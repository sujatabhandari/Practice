{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0976e7d79bba409a8ad2a3324ba0ac8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7147eec910444256b9f22726e7bce349",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdfade437dd14c64968d9832d481f637",
              "IPY_MODEL_ace770f78e734c93a4ddb136b109856c",
              "IPY_MODEL_f9eed2c35b6f4dada2b937b06f0eee6e"
            ]
          }
        },
        "7147eec910444256b9f22726e7bce349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdfade437dd14c64968d9832d481f637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_adcafd0096084d03868c5799f069d7de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Dl Completed...: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a61293aa0614a7da4239086e9429677"
          }
        },
        "ace770f78e734c93a4ddb136b109856c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_40fd2148012749888641790f501d2a28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ab84da13e0144ab8256a217f9628697"
          }
        },
        "f9eed2c35b6f4dada2b937b06f0eee6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1107f9ba795f4374b42f1ef0e1cf5cd6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:02&lt;00:00,  1.91 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ccd2c9de9b44d71b0f490dba44659b9"
          }
        },
        "adcafd0096084d03868c5799f069d7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a61293aa0614a7da4239086e9429677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40fd2148012749888641790f501d2a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ab84da13e0144ab8256a217f9628697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1107f9ba795f4374b42f1ef0e1cf5cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ccd2c9de9b44d71b0f490dba44659b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujatabhandari/Practice/blob/main/Bhandaris_Final_Project_CMPS325.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sujata Bhandari**\n",
        "\n",
        "**CMPS 325**\n",
        "\n",
        "**Final Project**\n",
        "\n"
      ],
      "metadata": {
        "id": "-xhT7aKQdZgK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFnfAnx7JKP"
      },
      "source": [
        "# Project One\n",
        "\n",
        "This project involves observing the progress of several different network architectures learning an image classification task.\n",
        "\n",
        "Note that each network does require the cells before Part One to have been run, but they do not require the networks before them to have run. So you can break up your work and not have to run all the networks just to re-run the last one.\n",
        "\n",
        "You should set the runtime type to GPU under the Runtime menu to speed the computations. Note that this may restrict you to having one notebook open at a time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9WjEWaM1GVz",
        "outputId": "8afdfada-78cc-4500-fb75-f719024c272f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_DPcmSw5teg"
      },
      "source": [
        "See a List of TensorFlow datasets at: https://knowyourdata-tfds.withgoogle.com/\n",
        "\n",
        "See how to load them:\n",
        "https://www.tensorflow.org/datasets/overview\n",
        "\n",
        "See data augmentation discussion: https://www.tensorflow.org/tutorials/images/data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YF3LzdL4PJv"
      },
      "source": [
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Datasets\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607,
          "referenced_widgets": [
            "0976e7d79bba409a8ad2a3324ba0ac8d",
            "7147eec910444256b9f22726e7bce349",
            "cdfade437dd14c64968d9832d481f637",
            "ace770f78e734c93a4ddb136b109856c",
            "f9eed2c35b6f4dada2b937b06f0eee6e",
            "adcafd0096084d03868c5799f069d7de",
            "2a61293aa0614a7da4239086e9429677",
            "40fd2148012749888641790f501d2a28",
            "6ab84da13e0144ab8256a217f9628697",
            "1107f9ba795f4374b42f1ef0e1cf5cd6",
            "8ccd2c9de9b44d71b0f490dba44659b9"
          ]
        },
        "id": "GT_9mP384THL",
        "outputId": "97456127-55d6-4742-909e-857fed5fe450"
      },
      "source": [
        "# Load in the tf_flowers dataset\n",
        "# Separate into training, testing and validation sets\n",
        "\n",
        "train_split = 'train[:80%]'\n",
        "validation_split = 'train[80%:90%]'\n",
        "test_split = 'train[90%:]'\n",
        "(ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=[train_split, validation_split, test_split],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "print(ds_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0976e7d79bba409a8ad2a3324ba0ac8d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "tfds.core.DatasetInfo(\n",
            "    name='tf_flowers',\n",
            "    version=3.0.1,\n",
            "    description='A large set of images of flowers',\n",
            "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
            "    }),\n",
            "    total_num_examples=3670,\n",
            "    splits={\n",
            "        'train': 3670,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@ONLINE {tfflowers,\n",
            "    author = \"The TensorFlow Team\",\n",
            "    title = \"Flowers\",\n",
            "    month = \"jan\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT3Sd8PsPapw"
      },
      "source": [
        "# Set some network parameters\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = ds_info.features['label'].num_classes\n",
        "# Size of each image dimension (width and height)\n",
        "img_size = 80\n",
        "# Size of a batch of images\n",
        "batch_size = 128\n",
        "\n",
        "# Function to apply to each image to prepare it for training and inference\n",
        "def process_img(image, label):\n",
        "  # Resize the images, pad if needed to preserve aspect ratio\n",
        "  #image = tf.image.resize_with_pad(image, img_size, img_size)\n",
        "  image = tf.image.resize(image, (img_size, img_size))\n",
        "  # Change from ints in range 0-255 to floats in range 0-1\n",
        "  image = tf.cast(image, tf.float32) / 255.\n",
        "  # Change label from integer to one-hot vector\n",
        "  label = tf.one_hot(label, num_classes)\n",
        "  return image, label\n",
        "\n",
        "# Setup data pipeline\n",
        "# Training images should be shuffled\n",
        "train = ds_train.map(\n",
        "    process_img, \n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).cache().shuffle(ds_info.splits[train_split].num_examples).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation data for tuning hyperparameters\n",
        "validation = ds_validation.map(\n",
        "    process_img, \n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Test data for final evaluation\n",
        "test = ds_test.map(\n",
        "    process_img, \n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcX4tXKMMfBm"
      },
      "source": [
        "## Part One - Basic Two-Layer Network\n",
        "\n",
        "Build the network and the train the model using the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scxW18fTaG2T"
      },
      "source": [
        "# Basic two-layer network\n",
        "two_layer_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(192, activation='relu'),\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "two_layer_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ir5jOzsVqat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce433b01-adfe-405d-ee32-e8a903e66ed3"
      },
      "source": [
        "# Train the model\n",
        "# Watch the training and validation accuracy over time\n",
        "# A notably higher training accuracy means overfitting\n",
        "epochs=10\n",
        "history = two_layer_model.fit(\n",
        "  train,\n",
        "  validation_data=validation,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.4154 - categorical_accuracy: 0.4278 - val_loss: 1.3027 - val_categorical_accuracy: 0.4387\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.5841 - categorical_accuracy: 0.4016 - val_loss: 1.4479 - val_categorical_accuracy: 0.3787\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.2639 - categorical_accuracy: 0.4728 - val_loss: 1.3360 - val_categorical_accuracy: 0.4332\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.3226 - categorical_accuracy: 0.4472 - val_loss: 1.3221 - val_categorical_accuracy: 0.4441\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.2451 - categorical_accuracy: 0.4663 - val_loss: 1.3076 - val_categorical_accuracy: 0.4496\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.2218 - categorical_accuracy: 0.4843 - val_loss: 1.3104 - val_categorical_accuracy: 0.4687\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.2492 - categorical_accuracy: 0.4758 - val_loss: 1.2603 - val_categorical_accuracy: 0.4796\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.1829 - categorical_accuracy: 0.5150 - val_loss: 1.2638 - val_categorical_accuracy: 0.4796\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 1.1953 - categorical_accuracy: 0.5194 - val_loss: 1.2891 - val_categorical_accuracy: 0.4986\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 1.1797 - categorical_accuracy: 0.5238 - val_loss: 1.4952 - val_categorical_accuracy: 0.4196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOvq2EqDwN4l",
        "outputId": "aad6d9b4-b3c5-4ba9-ce82-c824729868fe"
      },
      "source": [
        "# How big is this model?\n",
        "two_layer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 19200)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 192)               3686592   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                12352     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,699,269\n",
            "Trainable params: 3,699,269\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmpCIreENB74"
      },
      "source": [
        "### Questions\n",
        "1. How did the network do? What were the final training and validation accuracies?\n",
        "**The netwrok did not do very well as the training and validation accuracies were not that high. The accuracy for final training was about 52% whereas the accuracy for validation was about 42%.**\n",
        "2. Is there evidence of overfitting?\n",
        "**Yes, there is a clear evidence of overfitting as my training accuracy was higher than my validation accuracy from Epoch2 through Epoch6. The training accuracies from Epoch 2 to Epoch 6 were higher(40%, 47%,44.7%,46%,and 48%)respectively compared to their validation accuracies of(37%, 43%,44.4%,45%,and 47%)respectively.**\n",
        "3. Why do you think the network had trouble? What are the limitations of this architecture?\n",
        "**I think that the network had trouble because the data was non-parametric or non-linear. Similarly, I think that the basic two-layer network did not work well for this training model with the data. Some of the limitations of this architecture are:\n",
        "1) Slower to train the model\n",
        "2) Despite of it running slow, there can be seen overfitting in the results that are not helpful at all.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZWrtKHQNCIZ"
      },
      "source": [
        "## Part Two - Basic Convolutional Network\n",
        "\n",
        "Build the network and the train the model using the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1uCQ98hqeip"
      },
      "source": [
        "# A basic convolutional neural network with two sets of convolution/pooling layers\n",
        "# Notice that typically only one hidden dense layer is used\n",
        "basic_conv_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(24, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(192, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "basic_conv_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yypyYUBql4H",
        "outputId": "636bf23e-79b4-46df-de3f-a6f276972c06"
      },
      "source": [
        "# Train the model\n",
        "# Watch the training and validation accuracy over time\n",
        "# A notably higher training accuracy means overfitting\n",
        "epochs=10\n",
        "history = basic_conv_model.fit(\n",
        "  train,\n",
        "  validation_data=validation,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 2s 42ms/step - loss: 1.7366 - categorical_accuracy: 0.3505 - val_loss: 1.2650 - val_categorical_accuracy: 0.4768\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 1.2572 - categorical_accuracy: 0.4731 - val_loss: 1.1282 - val_categorical_accuracy: 0.5777\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 1s 32ms/step - loss: 1.1104 - categorical_accuracy: 0.5644 - val_loss: 1.0433 - val_categorical_accuracy: 0.6049\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 1s 32ms/step - loss: 1.0179 - categorical_accuracy: 0.6063 - val_loss: 1.0797 - val_categorical_accuracy: 0.5559\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 0.9375 - categorical_accuracy: 0.6379 - val_loss: 1.1958 - val_categorical_accuracy: 0.5014\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 0.8606 - categorical_accuracy: 0.6747 - val_loss: 0.9434 - val_categorical_accuracy: 0.6458\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 0.7921 - categorical_accuracy: 0.7074 - val_loss: 1.1033 - val_categorical_accuracy: 0.5777\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 1s 32ms/step - loss: 0.7426 - categorical_accuracy: 0.7377 - val_loss: 0.8852 - val_categorical_accuracy: 0.6403\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 1s 32ms/step - loss: 0.6567 - categorical_accuracy: 0.7595 - val_loss: 1.1231 - val_categorical_accuracy: 0.5913\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 1s 32ms/step - loss: 0.6252 - categorical_accuracy: 0.7715 - val_loss: 0.8755 - val_categorical_accuracy: 0.6730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbxJjJo8wUWG",
        "outputId": "4469d6b8-f364-4478-c92a-5508d0101301"
      },
      "source": [
        "# How big is this model?\n",
        "basic_conv_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_98 (Conv2D)          (None, 80, 80, 16)        448       \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 40, 40, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_99 (Conv2D)          (None, 40, 40, 24)        3480      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 20, 20, 24)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 9600)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 192)               1843392   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 5)                 965       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,848,285\n",
            "Trainable params: 1,848,285\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puqplpWWPGFB"
      },
      "source": [
        "### Questions\n",
        "1. How did the network do? What were the final training and validation accuracies?\n",
        "**The network did work very well as the training accuracy and validation accuracies were comparatively higher than the previous network. The final training accuracy was 77% whereas the validation accuracy was 67%.**\n",
        "\n",
        "2. Is there evidence of overfitting?\n",
        "**Yes, we can see a clear evidence of overfitting, my training accuracy was higher than the validation accuracy from Epoch 4 through Epoch10. The training accuracies from Epoch 4 through Epoch 10 were (60%, 63%,67% 70%, 73%,76%,77%)respectively compared to their validation accuracy of(55%,50%,64% 57%,64%,59%,67%)**\n",
        "3. What do convolutional networks do to improve learning?\n",
        "**The convolutional networks use small filter than the input, allowing it to be multipled various times by the input array. The small filter is designed to detect a particular feature anywhere in the image that ultimately creates a feature map and helps to improve learning. Other than having small filter, convolutional networks also have multiple layers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDKrUjoJPU-s"
      },
      "source": [
        "## Augmented Convolutional Neural Network\n",
        "\n",
        "Build the network and the train the model using the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d6PJuXsrG_S"
      },
      "source": [
        "# A convolutional neural network with some features to combat overfitting\n",
        "# Data augmentation spices up the input by applying random transformations\n",
        "# Dropout nulls out neurons randomly forcing the network to develop\n",
        "# redundancy and thus fit less to noise\n",
        "aug_conv_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(24, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(192, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "aug_conv_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa5Qbyg8rSAJ",
        "outputId": "a4ede2ad-e4f0-4852-ecb7-728d5de2a0ca"
      },
      "source": [
        "# Train the model\n",
        "# Watch the training and validation accuracy over time\n",
        "# A notably higher training accuracy means overfitting\n",
        "epochs=10\n",
        "history = aug_conv_model.fit(\n",
        "  train,\n",
        "  validation_data=validation,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 16s 86ms/step - loss: 1.6336 - categorical_accuracy: 0.3481 - val_loss: 1.6544 - val_categorical_accuracy: 0.2343\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 1.2957 - categorical_accuracy: 0.4584 - val_loss: 1.3815 - val_categorical_accuracy: 0.3597\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 1.1863 - categorical_accuracy: 0.5194 - val_loss: 1.1712 - val_categorical_accuracy: 0.5204\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 1s 35ms/step - loss: 1.1367 - categorical_accuracy: 0.5416 - val_loss: 1.1514 - val_categorical_accuracy: 0.5395\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 1s 38ms/step - loss: 1.1039 - categorical_accuracy: 0.5654 - val_loss: 1.1051 - val_categorical_accuracy: 0.5668\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 1.0720 - categorical_accuracy: 0.5691 - val_loss: 1.0355 - val_categorical_accuracy: 0.5995\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 1.0152 - categorical_accuracy: 0.5923 - val_loss: 1.0106 - val_categorical_accuracy: 0.6240\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 1.0041 - categorical_accuracy: 0.6083 - val_loss: 1.0230 - val_categorical_accuracy: 0.6022\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 0.9677 - categorical_accuracy: 0.6253 - val_loss: 0.9436 - val_categorical_accuracy: 0.6376\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.9663 - categorical_accuracy: 0.6264 - val_loss: 0.9182 - val_categorical_accuracy: 0.6621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_22UK_Uwb-e",
        "outputId": "50877cb8-e06f-489e-c9bc-99f91891b684"
      },
      "source": [
        "# Evaluate its performance on the test set\n",
        "print(\"Testing Results:\")\n",
        "aug_conv_model.evaluate(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Results:\n",
            "3/3 [==============================] - 1s 189ms/step - loss: 0.8726 - categorical_accuracy: 0.6403\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8726206421852112, 0.640326976776123]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YIR3aDPpGG"
      },
      "source": [
        "### Questions\n",
        "1. How did the network do? What were the final training and validation accuracies?\n",
        "**The network woked fairly good but the accuracy was not that high. The final training accuracy was about 62% whereas the final validation accuracy was about 66%.**\n",
        "2. Is there evidence of overfitting?\n",
        "**Yes, there is a clear evidence of overfitting as my training accuracy were way higher than the validation accuracy from Epoch 1 through Epoch4 except Epoch 3. The training accuracy from Epoch 1 through Epoch 4 except Epoch 3 were (35%,46%,,54.16%) respectively compared to the validation accuracy of (23%, 36%,53.9%).**\n",
        "3. Is the overfitting reduced compared to the previous network?\n",
        "**Yes. the overfitting reduced compared to the Basic Convolutional Network as, in the basic convolutional network we saw a lot of overfittings compared to this data. The previous network had 7 overfittings whereas the new one has only 3 overfittings.**\n",
        "4. How did the network do on the test data that it hadn't seen before? Was the accuracy comparable?\n",
        "**On the test data, the network gave us a loss of about 87.26%, and a accuracy of 64% however it did not gave us the validation accuracy. The loss that we got from the test data was low about 87.26% compared to what we got previously of 96.6%. The accuracy however, were only slightly different, we got a slightly higher accuracy of 64% compared to that of 62.6% causing a difference of about 1.4%.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ7aGId4P_nS"
      },
      "source": [
        "## Network with Pre-trained Feature Extractors\n",
        "\n",
        "Build the network and the train the model using the cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqb2qEqHMFAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4723babb-f129-4206-8672-3c75d0e8e464"
      },
      "source": [
        "# Load in the pre-trained model\n",
        "# Note that include_top=False means we are including just\n",
        "# the feature extractors and not the classification layers\n",
        "pre_trained_model = tf.keras.applications.inception_v3.InceptionV3(\n",
        "    input_shape=(img_size, img_size, 3), include_top=False)\n",
        "\n",
        "# Make sure we don't try to train these massive layers\n",
        "pre_trained_model.trainable = False\n",
        "\n",
        "# Build the model as before\n",
        "pre_trained_model = tf.keras.Sequential([\n",
        "  pre_trained_model,\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(192, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "pre_trained_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvVvJYdpSLsn",
        "outputId": "913e03bf-d98f-4a23-c9c0-22aa0cfc29b9"
      },
      "source": [
        "# Train the model\n",
        "# Watch the training and validation accuracy over time\n",
        "# A notably higher training accuracy means overfitting\n",
        "epochs=5\n",
        "history = pre_trained_model.fit(\n",
        "  train,\n",
        "  validation_data=validation,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "23/23 [==============================] - 13s 283ms/step - loss: 1.5481 - categorical_accuracy: 0.4864 - val_loss: 0.9606 - val_categorical_accuracy: 0.6294\n",
            "Epoch 2/5\n",
            "23/23 [==============================] - 2s 93ms/step - loss: 0.9107 - categorical_accuracy: 0.6553 - val_loss: 0.9560 - val_categorical_accuracy: 0.6458\n",
            "Epoch 3/5\n",
            "23/23 [==============================] - 2s 93ms/step - loss: 0.7159 - categorical_accuracy: 0.7384 - val_loss: 0.8930 - val_categorical_accuracy: 0.6621\n",
            "Epoch 4/5\n",
            "23/23 [==============================] - 2s 94ms/step - loss: 0.5871 - categorical_accuracy: 0.7830 - val_loss: 0.9716 - val_categorical_accuracy: 0.6294\n",
            "Epoch 5/5\n",
            "23/23 [==============================] - 2s 93ms/step - loss: 0.4839 - categorical_accuracy: 0.8338 - val_loss: 0.8398 - val_categorical_accuracy: 0.6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ruul0bUQ4r0",
        "outputId": "c4df5583-8de6-451b-a90a-9ace24af899c"
      },
      "source": [
        "pre_trained_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 1, 1, 2048)        21802784  \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 192)               393408    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 5)                 965       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,197,157\n",
            "Trainable params: 394,373\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkzpeCSERTJJ"
      },
      "source": [
        "### Questions\n",
        "1. How did the network do? What were the final training and validation accuracies?\n",
        "**The network worked pretty. The final training accuracy was 83% whereas the validation accuracy was 67%.**\n",
        "2. Is there evidence of overfitting?\n",
        "**Yes, there is a clear evidence of overfitting. As my training accuracy was much higher compared to my validation accuracy from Epoch 2 through Epoch 5. My training accuracy from Epoch 2 through Epoch 5 were (65%, 73.8%, 78.3%, 83.3%) respectively compared to validation accuracy of (64.5%,66%,62.9%, 67%)respectively.**\n",
        "3. Why might you include pre-trained layers? Are they improving performance?\n",
        "**We include pre-trained layers because it converges fastly thus saves time, and there is no need to create neural networks everytime and hence is more efficient. Yes, using the pre-trained layers did improve my performance. I got my highest training accuracy of 83.38% compared to other models.**\n",
        "4. How does the number of parameters (weights) compare to the previous networks (Hint: look at the summaries).\n",
        "**The number of parameters are way higher than the previous networks. The number of parameters that we have in this network is 22,197,157 compared to the previous ones of 3,699,269, and 1,848,285. We can also notice that in the previous network all of the parameters were trainable leaving the non trainable to be 0. However, in this network only 394,373 parameters are trainable and 21,802,784 parameters were not trainable.**\n"
      ]
    }
  ]
}